---
---

@string{aps = {American Physical Society,}}
@INPROCEEDINGS{JaiswalSAA,
  author={Jaiswal, Prateek and Honnappa, Harsha and Pasupathy, Raghu},
  booktitle={2018 Winter Simulation Conference (WSC)}, 
  title={Optimal Allocations for Sample Average Apporximation}, 
  year={2018},
  volume={},
  number={},
  pages={1874-1885},
  doi={10.1109/WSC.2018.8632258}}


@INPROCEEDINGS{RuixinVAC,
  author={Wang, Ruixin and Jaiswal, Prateek and Honnappa, Harsha},
  booktitle={2020 Winter Simulation Conference (WSC)}, 
  title={Estimating Stochastic Poisson Intensities Using Deep Latent Models}, 
  year={2020},
  volume={},
  number={},
  pages={596-607},
  doi={10.1109/WSC48552.2020.9383967}}


@article{JaiswalRenyi, 
author = {Jaiswal, Prateek and Rao, Vinayak and Honnappa, Harsha}, 
title = {Asymptotic Consistency of $\alpha$-R\'{e}nyi-Approximate Posteriors}, 
year = {2020}, 
issue_date = {January 2020}, 
publisher = {JMLR.org}, 
volume = {21}, 
number = {1}, 
issn = {1532-4435}, 
abstract = {We study the asymptotic consistency properties of α-R\'{e}nyi approximate posteriors, a class of variational Bayesian methods that approximate an intractable Bayesian posterior with a member of a tractable family of distributions, the member chosen to minimize the α-R\'{e}nyi divergence from the true posterior. Unique to our work is that we consider settings with α > 1, resulting in approximations that upperbound the log-likelihood, and consequently have wider spread than traditional variational approaches that minimize the Kullback-Liebler (KL) divergence from the posterior. Our primary result identifies sufficient conditions under which consistency holds, centering around the existence of a 'good' sequence of distributions in the approximating family that possesses, among other properties, the right rate of convergence to a limit distribution. We further characterize the good sequence by demonstrating that a sequence of distributions that converges too quickly cannot be a good sequence. We also extend our analysis to the setting where α equals one, corresponding to the minimizer of the reverse KL divergence, and to models with local latent variables. We also illustrate the existence of good sequence with a number of examples. Our results complement a growing body of work focused on the frequentist properties of variational Bayesian methods.}, journal = {J. Mach. Learn. Res.}, month = {jan}, articleno = {156}, numpages = {42}, keywords = {Bayesian computation, α-R\'{e}nyi divergence, variational inference, asymptotic consistency} 
}

@inproceedings{JaiswalBJCCPAABI,
  title={Variational {B}ayesian Methods for Stochastically Constrained System Design Problems},
  author={Jaiswal, Prateek and Honnappa, Harsh and Rao, Vinayak A},
  booktitle={Symposium on Advances in Approximate {B}ayesian Inference},
  pages={1--12},
  year={2020},
  organization={PMLR}
}

@inproceedings{jaiswal2020statistical,
  title={Statistical inference for approximate {B}ayesian optimal design},
  author={Jaiswal, Prateek and Honnappa, Harsha},
  booktitle={2020 Winter Simulation Conference (WSC)},
  pages={2138--2148},
  year={2020},
  organization={IEEE}
}

@article{JaiswalLCVB,
  doi = {10.1002/sta4.258},
  year = {2020},
  month = feb,
  publisher = {Wiley},
  volume = {9},
  number = {1},
  author = {Prateek Jaiswal and Harsha Honnappa and Vinayak A. Rao},
  title = {Asymptotic consistency of loss-calibrated variational {{Bayes}}},
  journal = {Stat}
}

@article{jaiswalRSVB,
  title={Risk-Sensitive Variational {{B}ayes}: Formulations and Bounds},
  author={Jaiswal, Prateek and Honnappa, Harsha and Rao, Vinayak A},
  journal={arXiv preprint arXiv:1903.05220v3},
  year={2019}
}

@article{JaiswalBJCCP,
author = {Jaiswal, Prateek and Honnappa, Harsha and Rao, Vinayak A.},
title = {Bayesian Joint Chance Constrained Optimization: Approximations and Statistical Consistency},
journal = {SIAM Journal on Optimization},
volume = {33},
number = {3},
pages = {1968-1995},
year = {2023},
doi = {10.1137/21M1430005},
URL = { https://doi.org/10.1137/21M1430005},
eprint = { https://doi.org/10.1137/21M1430005},
    abstract = { Abstract. This paper considers data-driven chance-constrained stochastic optimization problems in a Bayesian framework. Bayesian posteriors afford a principled mechanism to incorporate data and prior knowledge into stochastic optimization problems. However, the computation of Bayesian posteriors is typically an intractable problem and has spawned a large literature on approximate Bayesian computation. Here, in the context of chance-constrained optimization, we focus on the question of statistical consistency (in an appropriate sense) of the optimal value, computed using an approximate posterior distribution. To this end, we rigorously prove a frequentist consistency result demonstrating the convergence of the optimal value to the optimal value of a fixed, parameterized constrained optimization problem. We augment this by also establishing a probabilistic rate of convergence of the optimal value. We also prove the convex feasibility of the approximate Bayesian stochastic optimization problem. Finally, we demonstrate the utility of our approach on an optimal staffing problem for an M/M/c queueing model. }
}



